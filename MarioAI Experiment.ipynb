{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8358fdc1-6907-433e-bb15-49b5084fb984",
   "metadata": {},
   "source": [
    "# 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e1346-3e91-42bd-b6b4-8d8ea3f92532",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nes-py gym-super-mario-bros\n",
    "!pip install stable-baselines3[extra]\n",
    "\n",
    "# Only works on NVIDIA GPUs with CUDA installed\n",
    "!conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e7100-ddef-4a6e-b307-1f175f359cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Environment\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecTransposeImage\n",
    "\n",
    "# For the Learning Model\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236fe75f-2146-48b6-8d65-9a0ed7b971ea",
   "metadata": {},
   "source": [
    "# 2. Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f5630-4ced-4a0f-b7f5-84f5e6e0a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'SuperMarioBros-v3'\n",
    "SAVE_FREQ = 500000\n",
    "EVAL_FREQ = 100000\n",
    "TOT_TIMESTEPS = 6000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d495a5-5a4d-4bc5-8570-c2c10c35fb69",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Create and Preprocess Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a1f6b-ec58-45ab-b935-39d0c8e6a159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_and_preprocess_env():\n",
    "    env = gym_super_mario_bros.make(ENV_NAME)\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "    env = GrayScaleObservation(env, keep_dim=True)\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(env, 4, channels_order='last')\n",
    "    env = VecTransposeImage(env)\n",
    "    return env\n",
    "    \n",
    "train_env = create_and_preprocess_env()\n",
    "eval_env = create_and_preprocess_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591c7b0-a4ea-41e3-9e8d-73eb407f8d35",
   "metadata": {},
   "source": [
    "Note: There are now two environments, one for training and one for evaluation. This is because most learning models use exploration noise during training, and using a separate environment for evaluation prevents any conflicts with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3c668-4f6d-4840-bb59-1328e25d32a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Create and Train Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c3e6d-8d61-474c-91db-2e76057e8723",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A. Control Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb9cad-82b2-4c90-98ac-d86acc39cbb7",
   "metadata": {},
   "source": [
    "Note: The hyperparameters for Control Agent are set to the default values. The Control Agent is equivalent to the agent created in 'MarioAI'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7eb9d-c1cd-4a2a-8e4f-a96dfee3df00",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013240f9-c60d-41cd-8a15-98bbbc45386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Control/Saved Models/'\n",
    "log_path = './Control/Logs/'\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq = SAVE_FREQ, \n",
    "    save_path = save_path,\n",
    "    name_prefix = 'Control')\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    eval_freq = EVAL_FREQ, \n",
    "    best_model_save_path = save_path)\n",
    "\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37433f12-e2b9-4d19-b337-284524a4e708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', train_env, verbose=1, tensorboard_log=log_path,\n",
    "            # These are the default values\n",
    "            learning_rate = 3e-4,\n",
    "            n_steps = 2048,\n",
    "            batch_size = 64,\n",
    "            n_epochs = 10)\n",
    "\n",
    "model.learn(total_timesteps=TOT_TIMESTEPS, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71d9ae-1e4a-4025-9062-983c381e056f",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c69ffc4-357c-44d8-9506-3370255746b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state = eval_env.reset()\n",
    "while True:\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = eval_env.step(action)\n",
    "    eval_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119f1da-561a-4bbb-84f8-64d7ad02daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd67e3-c71a-4314-9bde-b96a07930b66",
   "metadata": {},
   "source": [
    "## B. Experiment Agent 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca57106-02de-4793-944c-91992738a11e",
   "metadata": {},
   "source": [
    "Note: The hyperparameters for Experiment Agent 1 were chosen based on the auto-tuned hyperparameters for the CarRacing-v0 environment found here: <u>https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml#LC320</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b6d37-d4e6-47a6-9986-22242b927596",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b43498-f0dc-4222-a25c-d1ab542fbb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Experiment1/Saved Models/'\n",
    "log_path = './Experiment1/Logs/'\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq = SAVE_FREQ, \n",
    "    save_path = save_path,\n",
    "    name_prefix = 'Experiment1')\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    eval_freq = EVAL_FREQ, \n",
    "    best_model_save_path = save_path)\n",
    "\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799f44e-d3a6-4ab6-8575-f9c941cf3ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', train_env, verbose=1, tensorboard_log=log_path,\n",
    "            learning_rate = 3e-5,\n",
    "            n_steps = 512,\n",
    "            batch_size = 128,\n",
    "            n_epochs = 20)\n",
    "\n",
    "model.learn(total_timesteps=TOT_TIMESTEPS, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2b7b3-bb8b-46da-b06e-05203102b72b",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be6411-1e61-49c1-94db-e69b48fb868e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state = eval_env.reset()\n",
    "while True:\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = eval_env.step(action)\n",
    "    eval_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9849850-63be-4f34-b77f-aed25ec23cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab18247-c959-4f5c-b84a-58df11d4ffa5",
   "metadata": {},
   "source": [
    "## C. Experiment Agent 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af4ad4d-a061-4351-94c6-037853616b61",
   "metadata": {},
   "source": [
    "Note: The hyperparameters for Experiment Agent 2 were chosen based on the auto-tuned hyperparameters for the Atari environments found here: <u>https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml#LC1</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ebc78f-ea0e-410e-810b-35d751948175",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5976213-a2ee-498a-aa13-1550ce820ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Experiment2/Saved Models/'\n",
    "log_path = './Experiment2/Logs/'\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq = SAVE_FREQ, \n",
    "    save_path = save_path,\n",
    "    name_prefix = 'Experiment2')\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    eval_freq = EVAL_FREQ, \n",
    "    best_model_save_path = save_path)\n",
    "\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d7751-1688-4d88-a047-4c0b6182fab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', train_env, verbose=1, tensorboard_log=log_path,\n",
    "            learning_rate = 2.5e-4,\n",
    "            n_steps = 128,\n",
    "            batch_size = 256,\n",
    "            n_epochs = 4)\n",
    "\n",
    "model.learn(total_timesteps=TOT_TIMESTEPS, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ebd450-8d7f-4fb7-8598-00515b70de29",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29886f-def7-4aa7-a6a1-628c814c83f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state = eval_env.reset()\n",
    "while True:\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = eval_env.step(action)\n",
    "    eval_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9355f99-d0a3-4504-9c47-a11084056a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dede638-4f15-42e8-8d6a-fb99c837decc",
   "metadata": {},
   "source": [
    "## D. Experiment Agent 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13afa9f7-1c87-4c63-aa28-02da92eb9ca7",
   "metadata": {},
   "source": [
    "Note: The hyperparameters for Experiment Agent 3 were chosen based on my best estimations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010861a1-4c50-4308-bd9c-8d14bfad671b",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1b321-1d22-4920-af29-7e3b6f604213",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Experiment3/Saved Models/'\n",
    "log_path = './Experiment3/Logs/'\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq = SAVE_FREQ, \n",
    "    save_path = save_path,\n",
    "    name_prefix = 'Experiment3')\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    eval_freq = EVAL_FREQ, \n",
    "    best_model_save_path = save_path)\n",
    "\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e13f9-ebe9-4c52-b718-c843df40cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', train_env, verbose=1, tensorboard_log=log_path,\n",
    "            learning_rate = 5e-7,\n",
    "            n_steps = 512,\n",
    "            batch_size = 128,\n",
    "            n_epochs = 15)\n",
    "\n",
    "model.learn(total_timesteps=TOT_TIMESTEPS, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88002bad-5642-440a-a5c8-d77652d6b02a",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8719ce-d6a3-4224-a4fd-433f438712e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state = eval_env.reset()\n",
    "while True:\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = eval_env.step(action)\n",
    "    eval_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eada2-fd4b-4cc5-8ddc-7edb37bbb8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0279a-9144-4167-9e91-e74773090b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
