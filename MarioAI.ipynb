{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59eb199a-5f53-4a1d-8606-223266f61c81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Load and Test the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc5245-fc81-4888-a109-eeaed4b60731",
   "metadata": {},
   "source": [
    "<em>Load the base environment, the Super Mario Bros video game, and test that it works.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60556b91-4659-4e7a-8040-b4cbb6813ab6",
   "metadata": {},
   "source": [
    "Note: The <a href=\"https://pypi.org/project/gym-super-mario-bros/\"><u>Super Mario Bros environment</u></a> was created by Christian Kauten using OpenAI GYM and the Nes-py emulator. <a href=\"https://gym.openai.com/\"><u>OpenAI GYM</u></a> is a popular framework that aims to standardize environments for reinforced learning. It features many pre-built environments but also allows for custom environments. <a href=\"https://pypi.org/project/nes-py/\"><u>Nes-py</u></a> is an NES emulator designed for custom OpenAI GYM environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02378bc5-59e1-4e36-8e69-33cbc1e11294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the environment\n",
    "!pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491ed5a-2492-4b3e-be28-fa64dc5b74f3",
   "metadata": {},
   "source": [
    "Note: If you receive and error during installation, please install <a href=\"https://visualstudio.microsoft.com/vs/community/\"><u>Visual Studio</u></a> with the \"Desktop Development with C++\" workload and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65762d-02e7-44cb-b001-efd2fd9e5775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the environment\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2cf2c-e474-4539-b34a-50b3265413ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c865310-3af4-4121-bf0b-819ccb65657f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the base environment\n",
    "done = True # Create a 'Done' flag which determines whether to restart the game or not\n",
    "\n",
    "for step in range(2000): # Loop through each frame in the game\n",
    "    if done:\n",
    "        state = env.reset() # Start the game\n",
    "    state, reward, done, info = env.step(env.action_space.sample()) # Do a random action\n",
    "    env.render() # Display the game\n",
    "\n",
    "env.close() # Close the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d7c2a-a29a-4d01-9a53-c07f2338deb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Preprocess the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f457d-738e-4638-9601-fd259bdfef61",
   "metadata": {},
   "source": [
    "<em>Preprocess the base environment so the AI agent can train effectively.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa3b1a2-4bd8-42da-bb80-2ebfdcd0993d",
   "metadata": {},
   "source": [
    "Note: In its current state, the base environment is complex and hard to learn in. Simplyfying the environment will enable the AI agent to train effectively. This step serves the same purpose as preprocessing data for supervised or unsupervised learning. In the case of reinforced learning, the data is taken from the environment, so the environment itself must be preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01268c-941c-4b0e-936c-d7dc5e80dbc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Nes-py for controller support\n",
    "!pip install nes-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80fdd08-f5e4-4f23-9ea1-9d7ef97511bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Stable Baselines for reinforced learning resources\n",
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5662d11-f386-4511-b507-a9b26c5ea86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the Joypad wrapper to emulate an NES controller\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Import simplified movement so the AI agent has less actions to take\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "# Import grayscaling to remove color\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "# Import Vectorization wrappers to improve training performance\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "\n",
    "# Import MatplotLib to demonstrate the preprocessing changes\n",
    "import matplotlib.pyplot as plt\n",
    "# Matplotlib doesn't work for some reason unless 'KMP_DUPLICATE_LIB_OK' is set to True\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea224976-3600-4ac3-9440-85b4857ad474",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A. Simplify Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f87469-9976-4cff-ae3b-bd1a34b2c35c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the actions the AI Agent can take\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b29379-d88b-4031-8cf3-ff29d2ac37ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simplify the actions the AI Agent can take\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d3af2-cbd0-4743-aa7a-4e6120968d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the actions the AI Agent can take after simplyfying the actions\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc51a8f-9fce-4a73-94a4-f5b67bcf9762",
   "metadata": {},
   "source": [
    "Note: Notice how the action space went from 256 to 7. Originally, the AI Agent could take 256 actions, but they can only take 7 actions now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c883083a-f881-4bab-81f2-b3976d55a256",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B. Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bb1de-ef99-4648-b6e2-ec205953c241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show what the environment currently looks like and the state shape\n",
    "state = env.reset()\n",
    "plt.imshow(state)\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bd027-4c5c-450d-b066-98578a630a7a",
   "metadata": {},
   "source": [
    "Note: If you receive an \"access violation\" error, restart the kernel, create the environment again, but do NOT test the environment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08557e6-ae3b-450b-9cd6-6ec82028b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grayscale the environment\n",
    "env = GrayScaleObservation(env, keep_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63a09c-54c4-441e-92cb-73647e278a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show what the environment looks like and the state shape after grayscaling\n",
    "state = env.reset()\n",
    "plt.imshow(state)\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c8ac2-8694-4962-b188-fad5274292e2",
   "metadata": {},
   "source": [
    "Note: Notice how the color changes. Also, notice how the state shape shrunk. Originally, there were 3 color values, RGB, now there is only 1, gray."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0bd34-d322-4d3e-b5d8-cd0106666af4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### C. Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9692e-cf0e-4426-9ac3-8a197e8dbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the state shape\n",
    "state = env.reset()\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51911f8-82be-4183-91f3-f5489d2b70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the Environment\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e16bd15-80e8-44cf-8923-a452c1270fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the state shape after vectorizing\n",
    "state = env.reset()\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961279a3-12a3-4d8f-bfe1-baa23e637c37",
   "metadata": {},
   "source": [
    "Note: Notice how the state shape changes. By vectorizing, the AI Agent can take into account 4 frames at a time. This increases training efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46292de9-d8a0-4d00-afe0-1ee80b436536",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Train the AI Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02ab53-eec6-42c6-8730-2ec26f2c09d3",
   "metadata": {},
   "source": [
    "<em>Train the AI Agent using a complex model that implements the <a href=\"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\"><u>PPO learning algorithm</u></a> and the <a href=\"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html?highlight=cnnpolicy#stable_baselines3.ppo.CnnPolicy\"><u>Convolutional Neural Network policy</u></a> from <a href=\"https://stable-baselines3.readthedocs.io/en/master/\"><u>Stable Baselines3</u></a>.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609420b2-f902-4d18-a61a-0ea9ba3ba390",
   "metadata": {},
   "source": [
    "Note: PPO, or Proximal Policy Optimization, is a recent popular learning algorithm for reinforced learning. It is a type of policy gradient algorithm and aims to optimize its policy (discussed below). The authors of the <a href=\"https://arxiv.org/abs/1707.06347\"><u>Proximal Policy Optimization Algorithms</u></a> paper claim that PPO effectively balances implementation, sample complexity, and tuning. They found that PPO performs comparable or better than other popular learning algorithms, such as <a href=\"https://arxiv.org/abs/1611.01224\"><u>ACER</u></a> and <a href=\"https://arxiv.org/abs/1502.05477\"><u>TRPO</u></a>.\n",
    "\n",
    "Note: Learning algorithms, including PPO, use policies to alter their performance and operation depending on the environment. For example, cnn policy works well in environments where data is visual, like the Super Mario Bros environment. Mlp policy, on the other hand, works well with tabular environments, where the data is organized into tables. Now, cnn policy, or Convolutional Neural Network policy, works by using a convolutional neural network, a feed-forward neural network with 20 to 30 layers. Some of those layers are convolutional layers, which are optimized for analyzing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad13b2d-44e0-4d83-9b34-47ca1e860a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch to accelerate training by using my CUDA-supported NVIDIA GPU\n",
    "!conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0d7f4-9960-4dc6-aa86-89d2803005e5",
   "metadata": {},
   "source": [
    "Note: Installing PyTorch is not necessary and only works with NVIDIA GPU's that have <a href=\"https://developer.nvidia.com/cuda-downloads\"><u>CUDA</u></a> installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed00b439-d6e9-4f56-a923-69e5d2f02f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PPO to use for the learning algorithm\n",
    "from stable_baselines3 import PPO\n",
    "# Import a callback for saving models\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c718c32a-2461-49c3-bc78-8bddec3f6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for saved models\n",
    "save_path = './Saved Models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44e085-f49f-4b1d-8509-d0da2bd6252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callback for saving the model\n",
    "callback = CheckpointCallback(save_freq = 25000, save_path = save_path, name_prefix = 'MarioAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291503e-b280-4f29-899d-d54c3caa5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = PPO('CnnPolicy', env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21dc2ae-9596-4fe5-84f4-7e9a8192f58d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model for 2,000,000 timesteps\n",
    "model.learn(total_timesteps=2000000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd772dc-8513-4e4e-bb2c-641cf2810eef",
   "metadata": {},
   "source": [
    "Note: If you reveive an \"access violation\" error, restart the Kernel, create and preprocess the environment again, but do NOT test the environment again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53793e28-d776-41ac-ba7e-4d8f47251fd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Evaluate the AI Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51fa9b-b5ed-4c8a-a3cf-4be47368838b",
   "metadata": {},
   "source": [
    "<em>Load a model and watch the AI Agent play.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da891a7c-f2f4-4d54-9497-77d014e41d4c",
   "metadata": {},
   "source": [
    "Note: The below steps are set up to load and run the pre-trained AI agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48960da3-3809-4f80-93b7-fcbfdec60603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = PPO.load('./Saved Models/Control_6000000_steps', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dddbe1-e054-4533-a3db-c1fd8094e389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the game\n",
    "state = env.reset()\n",
    "# Loop through the game\n",
    "while True:\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b450c3-5e68-4903-8559-60d0fdc46c5f",
   "metadata": {},
   "source": [
    "Note: If you reveive an \"access violation\" error, restart the Kernel, create and preprocess the environment again, but do NOT test the environment again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6f8df-cab5-4bf3-810d-17e316aede7c",
   "metadata": {},
   "source": [
    "Note: To stop the game, interrupt the kernel for the above step (by clicking the square at the top of the screen) and close the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5e61b-d536-4417-8091-22f5e007b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the game\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ccde3-6021-4818-8bca-0a779acc7991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
