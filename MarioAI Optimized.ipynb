{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8358fdc1-6907-433e-bb15-49b5084fb984",
   "metadata": {},
   "source": [
    "# 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e1346-3e91-42bd-b6b4-8d8ea3f92532",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nes-py gym-super-mario-bros\n",
    "!pip install stable-baselines3[extra]\n",
    "\n",
    "# Only works on NVIDIA GPUs with CUDA installed\n",
    "!conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e7100-ddef-4a6e-b307-1f175f359cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Environment\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecTransposeImage\n",
    "\n",
    "# For the Learning Model\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d495a5-5a4d-4bc5-8570-c2c10c35fb69",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Create and Preprocess Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a1f6b-ec58-45ab-b935-39d0c8e6a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'SuperMarioBros-v3'\n",
    "\n",
    "def create_and_preprocess_env(env_name):\n",
    "    env = gym_super_mario_bros.make(env_name)\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "    env = GrayScaleObservation(env, keep_dim=True)\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(env, 4, channels_order='last')\n",
    "    env = VecTransposeImage(env)\n",
    "    return env\n",
    "    \n",
    "train_env = create_and_preprocess_env(env_name)\n",
    "eval_env = create_and_preprocess_env(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591c7b0-a4ea-41e3-9e8d-73eb407f8d35",
   "metadata": {},
   "source": [
    "Note: There are now two environments, one for training and one for evaluation. This is because most learning models use exploration noise during training, and using a separate environment for evaluation prevents any conflicts with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3c668-4f6d-4840-bb59-1328e25d32a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Create and Train the Optimized Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9a9d4-9fb8-4d9a-808e-f08093fe6d86",
   "metadata": {},
   "source": [
    "Note: I optimized the original MarioAI agent with hyperparameter tuning in 'MarioAI Experiment'. There, I trained four agents with different hyperparameter values. I chose the values with help from <a href=\"https://stable-baselines3.readthedocs.io/en/master/guide/rl_zoo.html\"><u>Stable Baselines3 Zoo</u></a>, which features auto-tuned hyperparameters for various types of learning models on popular environments. They did not have auto-tuned hyperparameters for the Super Mario Bros environment, but I reviewed similar environments. Regardless, once all agents had trained, I evaluated their performance over time with <a href=\"https://www.tensorflow.org/tensorboard/\"><u>TensorBoard</u></a>. I also evaluated their performance by watching them play the game. Finally, I input the hyperparameter values of the agent that performed the best here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013240f9-c60d-41cd-8a15-98bbbc45386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Saved Models/'\n",
    "log_path = './Logs/'\n",
    "\n",
    "# Save the model every 200,000 timesteps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq = 200000, \n",
    "    save_path = save_path,\n",
    "    name_prefix = 'Optimized')\n",
    "\n",
    "# Evaluate the model every 20,000 timesteps and save the best model\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    eval_freq = 20000, \n",
    "    best_model_save_path = save_path)\n",
    "\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37433f12-e2b9-4d19-b337-284524a4e708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', train_env, verbose=1, tensorboard_log=log_path,\n",
    "            # These are the optimized values\n",
    "            learning_rate = 3e-5,\n",
    "            n_steps = 512,\n",
    "            batch_size = 128,\n",
    "            n_epochs = 20)\n",
    "\n",
    "model.learn(total_timesteps=2000000, callback=callback) # Train the model for 2,000,000 timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22148c97-fb0a-444e-8a71-0d90274f3df7",
   "metadata": {},
   "source": [
    "Note: The optimized agent performed slightly better, more reliably, and smarter than the other agents as shown in Tensorboard and through observation of the agent playing. It consistently scored a reward between about 1,800 and 2,200 whereas other agents were either lower, less reliable, or both. This agent also adopted interesting strategies and managed to complete the first level of Super Mario Bros once. I believe the agent's success is primarily due to its learning rate as this has the largest potential to affect performance. From my experimentation, a learning rate of 3e-5 to 3e-7 or potentially lower is optimal and leads to high-performing, consistent agents within about 1,000,000 timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ee245-eab2-445f-aa7d-7b1b11d176e9",
   "metadata": {},
   "source": [
    "Note: I stored images of the Tensorboard results in \"MarioAI Experiment Tensorboard Results.docx\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19290e2-eca1-47cd-b783-7cf0cdf95718",
   "metadata": {},
   "source": [
    "Note: The experiment I used to optimize the hyperparameters could be improved. For an optimal experiment, I would adjust each hyperparameter individually to clearly see how each impacts performance. I would also test many different values of each hyperparameter. This, of course, would involve training many more agents. To account for this, I could have reduced the total number of timesteps each agent trains for. I have found that the agents usually stop improving, in terms of reward, around 1,000,000 timesteps. Therefore, instead of training each agent for 6,000,000 timesteps, I can train each agent for 2,000,000 timesteps without impacting their overall performance. I could also simply run the experiment for longer or acquire more computers to run multiple agents at one time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bee28d-71d6-491c-acde-93f8361be443",
   "metadata": {},
   "source": [
    "# 4. Evaluate the Optimized Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93531a0a-11c0-48b1-813f-02c9a96bd99b",
   "metadata": {},
   "source": [
    "Note: The below steps are set up to load and run the pre-trained optimized MarioAI agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f4136-85cb-4981-a545-9a11dbf61d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = PPO.load('./Saved Models/Experiment1_6000000_steps', env=eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd365177-859b-4d02-815e-4d0c3a22ef3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the game\n",
    "state = eval_env.reset()\n",
    "# Loop through the game\n",
    "while True:\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = eval_env.step(action)\n",
    "    eval_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b2990-a03c-4487-815e-852ccdd1f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the game\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef74794-6a0d-4672-b701-0ec3c73d5bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
